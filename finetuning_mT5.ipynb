{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8270662,"sourceType":"datasetVersion","datasetId":4910426}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def preprocess_function(text, summary):\n    inputs = tokenizer(str(text), max_length=512, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    labels = tokenizer(str(summary), max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n    return {\n        \"input_ids\": inputs.input_ids.flatten(),\n        \"attention_mask\": inputs.attention_mask.flatten(),\n        \"labels\": labels.input_ids.flatten()\n    }\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:40:25.117747Z","iopub.execute_input":"2024-04-28T11:40:25.118987Z","iopub.status.idle":"2024-04-28T11:40:25.124886Z","shell.execute_reply.started":"2024-04-28T11:40:25.118943Z","shell.execute_reply":"2024-04-28T11:40:25.124040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\n# Convert preprocessed_data to a list of dictionaries\ndataset_dict = {\n    \"input_ids\": preprocessed_data[\"input_ids\"].tolist(),\n    \"attention_mask\": preprocessed_data[\"attention_mask\"].tolist(),\n    \"labels\": preprocessed_data[\"labels\"].tolist(),\n}\n\n# Create a Dataset object from the list of dictionaries\ndataset = Dataset.from_dict(dataset_dict)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:42:03.582005Z","iopub.execute_input":"2024-04-28T11:42:03.582575Z","iopub.status.idle":"2024-04-28T11:42:03.743830Z","shell.execute_reply.started":"2024-04-28T11:42:03.582536Z","shell.execute_reply":"2024-04-28T11:42:03.742633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preprocessed_data = data.apply(lambda row: preprocess_function(row['Text'], row['Summary']), axis=1)\ndataset = Dataset.from_dict(preprocessed_data)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-28T11:40:27.307715Z","iopub.execute_input":"2024-04-28T11:40:27.308454Z","iopub.status.idle":"2024-04-28T11:41:06.662492Z","shell.execute_reply.started":"2024-04-28T11:40:27.308426Z","shell.execute_reply":"2024-04-28T11:41:06.661247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer, Trainer, TrainingArguments\nfrom datasets import Dataset","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:15:56.986902Z","iopub.execute_input":"2024-05-01T17:15:56.987671Z","iopub.status.idle":"2024-05-01T17:16:16.694229Z","shell.execute_reply.started":"2024-05-01T17:15:56.987640Z","shell.execute_reply":"2024-05-01T17:16:16.693277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initialize model and tokenizer**","metadata":{}},{"cell_type":"code","source":"model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\ntokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\", src_lang=\"te_IN\", tgt_lang=\"te_IN\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:16:21.214254Z","iopub.execute_input":"2024-05-01T17:16:21.214968Z","iopub.status.idle":"2024-05-01T17:16:58.759901Z","shell.execute_reply.started":"2024-05-01T17:16:21.214936Z","shell.execute_reply":"2024-05-01T17:16:58.758654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Loading train dataset**","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\n\n# Load CSV dataset\ndf = pd.read_csv(\"/kaggle/input/final-dataset/final_train.csv\")\n\n# Ensure \"Text\" and \"Summary\" columns are string type\ndf[\"Text\"] = df[\"Text\"].astype(str) \ndf[\"Summary\"] = df[\"Summary\"].astype(str)\n\n# Create a dictionary from DataFrame\ndataset_dict = {\"text\": df[\"Text\"].tolist(), \"summary\": df[\"Summary\"].tolist()}\n\n# Create Hugging Face Dataset\ndataset = Dataset.from_dict(dataset_dict)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:17:02.736726Z","iopub.execute_input":"2024-05-01T17:17:02.737118Z","iopub.status.idle":"2024-05-01T17:17:04.245287Z","shell.execute_reply.started":"2024-05-01T17:17:02.737088Z","shell.execute_reply":"2024-05-01T17:17:04.244056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Defining training arguments**","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"./results\",  # output directory\n    num_train_epochs=5,  # total number of training epochs\n    per_device_train_batch_size=4,  # batch size per device during training\n    save_steps=100000,  # number of updates steps before checkpoint saves\n    save_total_limit=2,  # limit the total amount of checkpoints\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:18:35.997228Z","iopub.execute_input":"2024-05-01T17:18:35.998034Z","iopub.status.idle":"2024-05-01T17:18:36.027535Z","shell.execute_reply.started":"2024-05-01T17:18:35.998007Z","shell.execute_reply":"2024-05-01T17:18:36.026542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Defining function to preprocess the dataset**","metadata":{}},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = tokenizer(examples[\"text\"], return_tensors=\"pt\", max_length=512, padding='max_length', truncation=True)\n    targets = tokenizer(examples[\"summary\"], return_tensors=\"pt\", max_length=128, padding='max_length', truncation=True)\n    return {\"input_ids\": inputs.input_ids, \"attention_mask\": inputs.attention_mask, \"labels\": targets.input_ids}\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:17:29.393704Z","iopub.execute_input":"2024-05-01T17:17:29.394503Z","iopub.status.idle":"2024-05-01T17:17:29.400649Z","shell.execute_reply.started":"2024-05-01T17:17:29.394462Z","shell.execute_reply":"2024-05-01T17:17:29.399554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocessing the dataset**","metadata":{}},{"cell_type":"code","source":"train_dataset = dataset.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:17:37.486606Z","iopub.execute_input":"2024-05-01T17:17:37.487551Z","iopub.status.idle":"2024-05-01T17:18:19.539598Z","shell.execute_reply.started":"2024-05-01T17:17:37.487516Z","shell.execute_reply":"2024-05-01T17:18:19.538518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Initializing the trainer**","metadata":{}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    #compute_metrics=compute_metrics,\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:18:43.467786Z","iopub.execute_input":"2024-05-01T17:18:43.468194Z","iopub.status.idle":"2024-05-01T17:18:43.489293Z","shell.execute_reply.started":"2024-05-01T17:18:43.468162Z","shell.execute_reply":"2024-05-01T17:18:43.488056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finetuning the model**","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T17:18:51.278393Z","iopub.execute_input":"2024-05-01T17:18:51.278813Z","iopub.status.idle":"2024-05-01T18:11:17.688890Z","shell.execute_reply.started":"2024-05-01T17:18:51.278783Z","shell.execute_reply":"2024-05-01T18:11:17.687525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Save the model**","metadata":{}},{"cell_type":"code","source":"trainer.save_model(\"./mt5_finetuned_model\")","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:57:35.362848Z","iopub.execute_input":"2024-04-28T12:57:35.363248Z","iopub.status.idle":"2024-04-28T12:57:38.219792Z","shell.execute_reply.started":"2024-04-28T12:57:35.363215Z","shell.execute_reply":"2024-04-28T12:57:38.218587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r mt5_model.zip /kaggle/working/mt5_finetuned_model","metadata":{"execution":{"iopub.status.busy":"2024-04-28T12:57:42.074034Z","iopub.execute_input":"2024-04-28T12:57:42.074888Z","iopub.status.idle":"2024-04-28T13:00:24.732274Z","shell.execute_reply.started":"2024-04-28T12:57:42.074854Z","shell.execute_reply":"2024-04-28T13:00:24.731105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r 1mt5_latest_gensum.zip /kaggle/working/generated_summaries.csv","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:45:01.917041Z","iopub.execute_input":"2024-05-01T18:45:01.917841Z","iopub.status.idle":"2024-05-01T18:45:03.070489Z","shell.execute_reply.started":"2024-05-01T18:45:01.917807Z","shell.execute_reply":"2024-05-01T18:45:03.069073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'1mt5_latest_gensum.zip')","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:45:06.900069Z","iopub.execute_input":"2024-05-01T18:45:06.901268Z","iopub.status.idle":"2024-05-01T18:45:06.915096Z","shell.execute_reply.started":"2024-05-01T18:45:06.901227Z","shell.execute_reply":"2024-05-01T18:45:06.912732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load the test dataset**","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\nimport pandas as pd\n\n# Load CSV dataset\ndf2 = pd.read_csv(\"/kaggle/input/final-dataset/final_test.csv\")\n\n# Ensure \"Text\" and \"Summary\" columns are string type\ndf2[\"Text\"] = df2[\"Text\"].astype(str) \ndf2[\"Summary\"] = df2[\"Summary\"].astype(str)\n\n# Create a dictionary from DataFrame\ndataset_dict2 = {\"text\": df2[\"Text\"].tolist(), \"summary\": df2[\"Summary\"].tolist()}\n\n# Create Hugging Face Dataset\ndataset2 = Dataset.from_dict(dataset_dict2)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:15:31.372318Z","iopub.execute_input":"2024-05-01T18:15:31.372768Z","iopub.status.idle":"2024-05-01T18:15:31.733674Z","shell.execute_reply.started":"2024-05-01T18:15:31.372734Z","shell.execute_reply":"2024-05-01T18:15:31.732718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Preprocess the test dataset**","metadata":{}},{"cell_type":"code","source":"test_dataset = dataset2.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:15:35.437629Z","iopub.execute_input":"2024-05-01T18:15:35.438485Z","iopub.status.idle":"2024-05-01T18:15:45.856438Z","shell.execute_reply.started":"2024-05-01T18:15:35.438442Z","shell.execute_reply":"2024-05-01T18:15:45.855161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Generate the summaries**","metadata":{}},{"cell_type":"code","source":"import torch\n\n# List to store generated summaries\ngenerated_summaries = []\n\n# Move model to appropriate device (CPU or GPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Iterate over the tokenized test dataset\ncount=0\nfor example in test_dataset:\n    # Convert input_ids and attention_mask to tensors and move to appropriate device\n    input_ids = torch.tensor([example[\"input_ids\"]]).to(device)\n    attention_mask = torch.tensor([example[\"attention_mask\"]]).to(device)\n    \n    # Generate prediction for the current example\n    prediction = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n    \n    # Decode the prediction and append it to the list\n    generated_summary = tokenizer.decode(prediction[0], skip_special_tokens=True)\n    generated_summaries.append(generated_summary)\n    count=count+1\n    #if(count>10):\n        #break","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:15:50.899203Z","iopub.execute_input":"2024-05-01T18:15:50.899996Z","iopub.status.idle":"2024-05-01T18:24:59.479281Z","shell.execute_reply.started":"2024-05-01T18:15:50.899961Z","shell.execute_reply":"2024-05-01T18:24:59.478161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Store the summaries in a csv file**","metadata":{}},{"cell_type":"code","source":"import csv\n\n# Assuming you have a list of generated summaries called generated_summaries\n#generated_summaries = ['Summary 1', 'Summary 2', 'Summary 3', ...]\n\n# Specify the file name\ncsv_file = '/kaggle/working/generated_summaries.csv'  # Saving in the working directory in Kaggle\n\n# Open the CSV file in write mode and write the summaries\nwith open(csv_file, 'w', newline='', encoding='utf-8') as file:\n    # Create a CSV writer object\n    writer = csv.writer(file)\n    \n    # Write each summary in the list as a row with a single column\n    for summary in generated_summaries:\n        writer.writerow([summary])\n\nprint(\"File created and data stored successfully.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:44:31.632898Z","iopub.execute_input":"2024-05-01T18:44:31.633325Z","iopub.status.idle":"2024-05-01T18:44:31.651384Z","shell.execute_reply.started":"2024-05-01T18:44:31.633297Z","shell.execute_reply":"2024-05-01T18:44:31.650224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Print the summaries**","metadata":{}},{"cell_type":"code","source":"for i in range(10):\n    print(f\"Text: {df2['Text'][i]}\")\n    print(f\"Original Summary: {df2['Summary'][i]}\")\n    print(f\"Generated Summary: {generated_summaries[i]}\\n\")\n    #print(f\"Generated Summary: {generated_summaries_telugu[i]}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:43:08.568216Z","iopub.execute_input":"2024-05-01T18:43:08.569120Z","iopub.status.idle":"2024-05-01T18:43:08.583366Z","shell.execute_reply.started":"2024-05-01T18:43:08.569085Z","shell.execute_reply":"2024-05-01T18:43:08.582177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Function to calculate BLEU score**","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n\ndef evaluate_summary(reference, generated):\n    # BLEU score\n    bleu_score = sentence_bleu([reference], generated)\n\n    # ROUGE score\n\n    return bleu_score","metadata":{"execution":{"iopub.status.busy":"2024-05-01T18:43:35.095537Z","iopub.execute_input":"2024-05-01T18:43:35.096276Z","iopub.status.idle":"2024-05-01T18:43:35.103005Z","shell.execute_reply.started":"2024-05-01T18:43:35.096247Z","shell.execute_reply":"2024-05-01T18:43:35.101809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Avg BLEU score calculation**","metadata":{}},{"cell_type":"code","source":"avg_bleu = 0\nbleu = 0\nfor i in range(10):\n    #print(f\"Text: {df2['Text'][i]}\")\n    #print(f\"Original Summary: {df2['Summary'][i]}\")\n    reference_summary = df2[\"Summary\"][i]\n    #print(f\"Generated Summary: {generated_summaries[i]}\\n\")\n    generated_summary = generated_summaries[i]\n    bleu = evaluate_summary(reference_summary, generated_summary)\n    avg_bleu += bleu\nprint(avg_bleu/10)","metadata":{"execution":{"iopub.status.busy":"2024-04-30T18:33:36.173278Z","iopub.execute_input":"2024-04-30T18:33:36.174025Z","iopub.status.idle":"2024-04-30T18:33:36.190676Z","shell.execute_reply.started":"2024-04-30T18:33:36.173988Z","shell.execute_reply":"2024-04-30T18:33:36.189717Z"},"trusted":true},"execution_count":null,"outputs":[]}]}